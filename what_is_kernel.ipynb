{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxssVU+pa0he7mkHB+3gtI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahab460/AI-ML-with-Python/blob/main/what_is_kernel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssy3SQUWv39h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WHAT IS KERNEL FUNCTION?**\n"
      ],
      "metadata": {
        "id": "HhM1F5YOv6eD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A kernel function is a mathematical function used in machine learning, particularly in Support Vector Machines (SVMs) and other algorithms, to compute the similarity between two data points in a transformed feature space without explicitly computing the transformation. The kernel function allows algorithms to operate in high-dimensional spaces without needing to directly work in that space, which is known as the \"kernel trick.\""
      ],
      "metadata": {
        "id": "tIhH6nsnwZlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Key Concepts**\n",
        "\n",
        "**Feature Space Transformation:**\n",
        "\n",
        "In some machine learning tasks, especially classification, data might not be linearly separable in the original feature space. To solve this, the data can be mapped to a higher-dimensional space where it becomes linearly separable.\n",
        "\n",
        "The function that performs this transformation is often denoted as\n",
        "𝜙(𝑥), where 𝑥 is a data point in the original space, and\n",
        "𝜙(𝑥) is its corresponding point in the higher-dimensional space.\n",
        "\n",
        "**Kernel Trick:**\n",
        "\n",
        "Instead of explicitly computing 𝜙(𝑥), which might be computationally expensive or even infeasible, the kernel trick allows us to calculate the dot product ⟨𝜙(𝑥𝑖),𝜙(𝑥𝑗)⟩ directly using a kernel function 𝐾(𝑥𝑖,𝑥𝑗).\n",
        "\n",
        "This means that the algorithm can operate as if it were in the higher-dimensional space without ever explicitly computing the coordinates in that space."
      ],
      "metadata": {
        "id": "X52BGgtNwPCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Common Kernel Functions**\n",
        "\n",
        "**Linear Kernel:**\n",
        "\n",
        "𝐾(𝑥𝑖,𝑥𝑗) = 𝑥𝑖𝑇𝑥𝑗\n",
        "\n",
        "This is equivalent to the standard dot product in the original feature space. It doesn't map data to a higher dimension but is useful for linear separable data.\n",
        "\n",
        "**Polynomial Kernel:**\n",
        "\n",
        "𝐾(𝑥𝑖,𝑥𝑗) = (𝑥𝑖𝑇𝑥𝑗 +𝑐)𝑑\n",
        "\n",
        "This maps the original features into a higher-dimensional space using polynomials. The degree of the polynomial 𝑑 and constant 𝑐 are parameters that can be adjusted.\n",
        "\n",
        "**Radial Basis Function (RBF) Kernel or Gaussian Kernel:**\n",
        "\n",
        "𝐾(𝑥𝑖,𝑥𝑗) = exp⁡(−∥𝑥𝑖−𝑥𝑗∥22 𝜎2)\n",
        "\n",
        "This kernel is based on the distance between two points and is commonly used in SVMs. It maps the data into an infinite-dimensional space, which often works well for non-linear problems.\n",
        "\n",
        "**Sigmoid Kernel:**\n",
        "\n",
        "𝐾(𝑥𝑖,𝑥𝑗) = tanh(𝜅𝑥𝑖𝑇𝑥𝑗 + 𝑐)\n",
        "\n",
        "This kernel function is similar to the activation function in neural networks and can be used in certain contexts for non-linear transformations."
      ],
      "metadata": {
        "id": "AJmgvNKEyUDg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Application in SVM**\n",
        "\n",
        "In SVM, the kernel function plays a critical role in finding the optimal hyperplane that separates the data into different classes. When using a kernel, the SVM algorithm effectively works in a higher-dimensional space where the data can be linearly separated, even if it’s not separable in the original space.\n",
        "\n",
        "## **Summary**\n",
        "\n",
        "**Kernel Function:** Computes the similarity between two data points in a\n",
        "potentially high-dimensional space without explicitly performing the transformation.\n",
        "\n",
        "\n",
        "**Kernel Trick:** Allows machine learning algorithms, especially SVM, to operate in high-dimensional spaces efficiently.\n",
        "\n",
        "\n",
        "**Types of Kernels:** Linear, Polynomial, RBF (Gaussian), and Sigmoid are some commonly used kernels, each with different properties suited for various types of data.\n",
        "\n",
        "\n",
        "By choosing an appropriate kernel function, you can effectively solve complex, non-linear problems by transforming them into linear problems in a higher-dimensional space."
      ],
      "metadata": {
        "id": "Njxkh6XqHilP"
      }
    }
  ]
}